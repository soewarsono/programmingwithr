---
title: How to create unigrams, bigrams and n-grams of App Reviews
author: AbdulMajedRaja RS
date: '2019-08-06'
slug: how-to-create-unigrams-bigrams-and-n-grams-of-app-reviews
categories:
  - Text
tags:
  - Data Visualization
  - NLP
  - r-bloggers
description: 'in R using tidytext'
topics: []
---

This is one of the frequent questions I've heard from the first timer NLP / Text Analytics - programmers (or as the world likes it to be called "Data Scientists"). 

### Prerequisite

For simplicity, this post assumes that you already know how to install a package and so you've got `tidytext` installed on your R machine. 

```{r eval=FALSE}
install.packages("tidytext")
```


### Loading the Library

Let's start with loading the `tidytext` library. 

```{r warning=FALSE, message=FALSE}
library(tidytext)
```

### Extracting App Reviews 

We'll use the R-package `itunesr` for downloading iOS App Reviews on which we'll perform Simple Text Analysis (unigrams, bigrams, n-grams). `getReviews()` funciton of `itunesr` helps us in extracting reviews of Medium iOS App. 

```{r warning=FALSE, message=FALSE}
library(itunesr)
library(tidyverse)

# Extracting Medium iOS App Reviews
medium <- getReviews("828256236","us",1)

```

### Overview of the extract App Reviews

As usual, we'll start with seeing what's `head` of the dataframe. 

```{r, warning=FALSE, message=FALSE}
head(medium) 
```

Now, we know that there are two Text Columns of our interest - `Title` and `Review`. 

To make our n-grams analysis a bit more meaningful, we'll extract only the positive reviews (5-star) to see what's good people are writing about Medium iOS App. To make a better sense of the filter we've to use let's see the split of `Rating`.

```{r}
table(medium$Rating)
```


So, 5-star is the major component in the text reviews we extract and we're good to go filtering only 5-star.We'll pick `Review` from that and also we'll specify it only for `Rating == 5`. Since we need a dataframe (or tibble) for tidytext to process it, we'll put these 5-star reviews as a new column in a new dataframe.


```{r}

reviews <- data.frame(txt = medium$Review[medium$Rating==5],
                      stringsAsFactors = FALSE)

```


### Tokens

Tokenization in NLP is the process of splitting a text corpus based on some splitting factor - It could be Word Tokens or Sentence Tokens or based on some advanced alogrithm to split a conversation. In this process, we'll just simply do word tokenization.

```{r}
reviews %>% 
  unnest_tokens(output = word, input = txt) %>% 
  head()
```

As you can see above, `unnest_tokens()` is the function that'll help us in this tokenization process. Since it supports `%>%`  pipe operator, the first argument of the function is a `dataframe` or `tibble`, the second argument `output` is the name of the output (new) column where the tokenized words are going to be put in. The third column `input` is where the input text is fed in. 


Now, this is what `unigram`s are for this Medium iOS App Reviews. As with many other data science projects, Data like this is not useful unless it's visualized in a way to look at insights. 

```{r}
reviews %>% 
  unnest_tokens(output = word, input = txt) %>% 
  count(word, sort = TRUE) 
```

Roughly, looking at the most frequently appeared unigram we end up with `the`,`i`,`and` and this is one of those places where we need to *remove stopwords*

### Stopword Removal

Fortunately, `tidytext` helps us in removing stopwords by having a dataframe of stopwords from multiple lexicons. With that, we can use `anti_join` for picking the words (that are present in the left df (`reviews`) but not present in the right df (`stop_words`)).

```{r}
reviews %>% 
  unnest_tokens(output = word, input = txt) %>% 
  anti_join(stop_words) %>% 
  count(word, sort = TRUE) 
```

With that stop word removal, now we can see better represenation of most frequently appearing unigrams in the reviews. 

### unigram Visualziation

We've got our data in the shape that we want so, let's go ahead and visualize it. To keep the pipeline intact, I'm not creating any temporary object to store the previous output and just simply continue using the same. Also too many bars (words) wouldn't make any sense (except resulting in a shabby plot), We'll filter taking the top 10 words

```{r}
reviews %>% 
  unnest_tokens(output = word, input = txt) %>% 
  anti_join(stop_words) %>% 
  count(word, sort = TRUE) %>% 
  slice(1:10) %>% 
  ggplot() + geom_bar(aes(word, n), stat = "identity", fill = "#de5833") +
  theme_minimal() +
  labs(title = "Top unigrams of Medium iOS App Reviews",
       subtitle = "using Tidytext in R",
       caption = "Data Source: itunesr - iTunes App Store")
```

### Bigrams & N-grams

Now that we've got the core code for unigram visualization set up. We can slightly modify the same - just by adding a new argument `n=2` and `token="ngrams"` to the tokenization process to extract n-gram. `2` for bigram and `3` trigram - or `n` of your interest. But remember, large n-values may not useful as the smaller values.

Doing this naively also has a catch and the catch is - the stop-word removal process we used above was using `anti_join` which wouldn't be supported in this process since we've a bigram (two-word combination separated by a space). So, we'll `separate` the word by `space` and then filter out the stop words in both `word1` and `word2` and then `unite` them back - which gives us the `bigram`  after stop-word removal. This is the process that you might have to carry out when you are dealing with n-grams. 
```{r}
reviews %>% 
  unnest_tokens(word, txt, token = "ngrams", n = 2) %>% 
  separate(word, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>% 
  unite(word,word1, word2, sep = " ") %>% 
  count(word, sort = TRUE) %>% 
  slice(1:10) %>% 
  ggplot() + geom_bar(aes(word, n), stat = "identity", fill = "#de5833") +
  theme_minimal() +
  coord_flip() +
  labs(title = "Top Bigrams of Medium iOS App Reviews",
       subtitle = "using Tidytext in R",
       caption = "Data Source: itunesr - iTunes App Store")
```


### Summary

This particular assignment that may not reveal some meaningful insights as we  started with less data, but this is really useful when you have a decent amount of text corpus and this simple analysis of unigram, bigram (n-gram analysis) can reveal something business-worthy (let's say in Customer Service, App Development or in multiple other use-cases). 

### References

+ [Tidy Text Mining](https://www.tidytextmining.com/)
+ [itunesr](https://github.com/amrrs/itunesr/)
